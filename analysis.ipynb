{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from network import resnet18\n",
    "\n",
    "from dataset import create_dataloaders_missing_class\n",
    "from config import dotdict\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "DEVICE = \"cuda:1\"\n",
    "from torchmetrics import Accuracy, ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load network\n",
    "net = resnet18(num_classes=10)\n",
    "net.load_state_dict(torch.load(\"/research/hal-gaudisac/unlearning/models/scratch_resnet18.pt\", map_location=torch.device(DEVICE))[\"model_state_dict\"])\n",
    "net.to(DEVICE)\n",
    "net.eval()\n",
    "\n",
    "#load unlearn network\n",
    "unlearn_net = resnet18(num_classes=10)\n",
    "unlearn_net.load_state_dict(torch.load(\"/research/hal-gaudisac/unlearning/models/NN_resnet18.pt\", map_location=torch.device(DEVICE))[\"model_state_dict\"])\n",
    "unlearn_net.to(DEVICE)\n",
    "unlearn_net.eval()\n",
    "\n",
    "finetune_net = resnet18(num_classes=10)\n",
    "finetune_net.load_state_dict(torch.load(\"models/finetune_resnet18.pt\", map_location=torch.device(DEVICE))[\"model_state_dict\"])\n",
    "finetune_net.to(DEVICE)\n",
    "finetune_net.eval()\n",
    "\n",
    "scrubs_net = resnet18(num_classes=10)\n",
    "scrubs_net.load_state_dict(torch.load(\"/research/hal-gaudisac/unlearning/models/scrubs_resnet18.pt\", map_location=torch.device(DEVICE))[\"model_state_dict\"])\n",
    "scrubs_net.to(DEVICE)\n",
    "scrubs_net.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#load data\n",
    "data_settings = dotdict({\"BATCH_SIZE\": 128, \"data\":{\"num_workers\": 4}, \"DATA_PATH\":\"data\",\"remove_class\":0})\n",
    "dataloaders = create_dataloaders_missing_class(data_settings)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "#to not load everything in memory\n",
    "dataloaders.forget.pin_memory=False\n",
    "dataloaders.val.pin_memory=False\n",
    "dataloaders.retain.pin_memory=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrain - forget : => 1.0\n",
      "retrain - retain : => 0.010311126708984375\n",
      "retrain - val : => 0.20452022552490234\n",
      "unlearn - forget : => 1.0\n",
      "unlearn - retain : => 0.006377756595611572\n",
      "unlearn - val : => 0.2014697790145874\n",
      "finetune - forget : => 0.9101999998092651\n",
      "finetune - retain : => 0.005533337593078613\n",
      "finetune - val : => 0.19994455575942993\n",
      "scrubs - forget : => 1.0\n",
      "scrubs - retain : => 0.008688867092132568\n",
      "scrubs - val : => 0.17970049381256104\n"
     ]
    }
   ],
   "source": [
    "acc = Accuracy(\"multiclass\",num_classes=10).to(DEVICE)\n",
    "cm = ConfusionMatrix(\"multiclass\",num_classes=10).to(DEVICE)\n",
    "\n",
    "\n",
    "cms = []\n",
    "for name,model in [(\"retrain\",net),(\"unlearn\",unlearn_net),(\"finetune\",finetune_net),(\"scrubs\",scrubs_net)]: #\n",
    "    for dl_name,dl in zip([\"forget\",\"retain\",\"val\"],[dataloaders.forget,dataloaders.retain,dataloaders.val]):\n",
    "        acc.reset()\n",
    "        cm.reset()\n",
    "        for batch_id, (inputs, targets,mask) in enumerate(dl):\n",
    "            if dl_name == \"val\":\n",
    "                inputs,targets = inputs[targets!=0], targets[targets!=0]\n",
    "\n",
    "            inputs,targets = inputs.to(DEVICE),targets.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = (outputs,targets)\n",
    "            acc.update(outputs,targets)\n",
    "            cm.update(outputs,targets)    \n",
    "        print(f\"{name} - {dl_name} : => {1 - acc.compute()}\")\n",
    "    val_cm = cm.compute().detach().cpu().numpy() \n",
    "    cms.append(val_cm/(val_cm.sum(axis=1,keepdims=True)+1))  #compute frobenius norm of confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0604955331644341, 0.049052444045379895, 0.09777196469661699)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(cms[0] - cms[1]),np.linalg.norm(cms[0] - cms[2]),np.linalg.norm(cms[0] - cms[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.06524467, 0.32496863, 0.06398996, 0.0727729 ,\n",
       "        0.0238394 , 0.01882058, 0.02634881, 0.31116688, 0.09284818],\n",
       "       [0.        , 0.89533417, 0.0037831 , 0.0075662 , 0.00252207,\n",
       "        0.00882724, 0.0037831 , 0.00126103, 0.01639344, 0.06052963],\n",
       "       [0.        , 0.        , 0.75371287, 0.04950495, 0.06683168,\n",
       "        0.04579208, 0.04084158, 0.02351485, 0.00990099, 0.00990099],\n",
       "       [0.        , 0.00623441, 0.05486284, 0.60723192, 0.05610973,\n",
       "        0.14962594, 0.05985037, 0.03366584, 0.01870324, 0.01371571],\n",
       "       [0.        , 0.00365408, 0.05359318, 0.05846529, 0.76248477,\n",
       "        0.02801462, 0.03775883, 0.04384896, 0.01096224, 0.00121803],\n",
       "       [0.        , 0.0063857 , 0.03703704, 0.17241379, 0.03192848,\n",
       "        0.68965517, 0.01532567, 0.03831418, 0.00255428, 0.0063857 ],\n",
       "       [0.        , 0.00371747, 0.03221809, 0.04337051, 0.02726146,\n",
       "        0.02973978, 0.84634449, 0.00743494, 0.00495663, 0.00495663],\n",
       "       [0.        , 0.00483676, 0.02902056, 0.02297461, 0.04353083,\n",
       "        0.05441354, 0.00483676, 0.82708585, 0.00241838, 0.01088271],\n",
       "       [0.        , 0.01520913, 0.0139417 , 0.01013942, 0.00887199,\n",
       "        0.00380228, 0.00253485, 0.00126743, 0.92648923, 0.01774398],\n",
       "       [0.        , 0.08279431, 0.01034929, 0.01164295, 0.00258732,\n",
       "        0.00388098, 0.00517464, 0.01034929, 0.02716688, 0.84605433]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cms[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hello:\n",
    "    def __init__(self) -> None:\n",
    "        self.sac = {\"a\":1}\n",
    "    def __getattr__(self, name):\n",
    "        return self.sac[name]\n",
    "    def update(self):\n",
    "        self.sac[\"a\"]=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sac': {'a': 2}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = Hello()\n",
    "h.update()\n",
    "\n",
    "h.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD CODE for ploting losses and understanding metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def accuracy(pds,tgts):\n",
    "    return torch.mean((torch.max(pds,dim=1)[1]==tgts).float())\n",
    "\n",
    "pds,tgts,ls = [],[],[]\n",
    "dataloaders.forget.pin_memory=False\n",
    "dataloaders.val.pin_memory=False\n",
    "dataloaders.retain.pin_memory=False\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for batch_id, (inputs, targets,mask) in enumerate(dataloaders.forget):\n",
    "    inputs,targets = inputs.to(DEVICE),targets.to(DEVICE)\n",
    "    outputs = net(inputs)\n",
    "    loss = (outputs,targets)\n",
    "    ls.append(loss_fn(outputs,targets).detach().cpu())\n",
    "    pds.append(F.softmax(outputs,dim=1))\n",
    "    tgts.append(targets)\n",
    "pds = torch.cat(pds)\n",
    "tgts = torch.cat(tgts)\n",
    "ls = torch.cat(ls)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "val_pds,val_tgts,val_ls = [],[],[]\n",
    "for batch_id, (inputs, targets,mask) in enumerate(dataloaders.retain):\n",
    "    inputs,targets = inputs.to(DEVICE),targets.to(DEVICE)\n",
    "    outputs = net(inputs)\n",
    "    val_ls.append(loss_fn(outputs,targets).detach().cpu())\n",
    "    val_pds.append(F.softmax(outputs,dim=1))\n",
    "    val_tgts.append(targets)\n",
    "    \n",
    "val_pds = torch.cat(val_pds)\n",
    "val_tgts = torch.cat(val_tgts)\n",
    "val_ls = torch.cat(val_ls)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "normal_pds,normal_tgts,normal_ls = [],[],[]\n",
    "for batch_id, (inputs, targets,mask) in enumerate(dataloaders.val):\n",
    "    inputs,targets = inputs.to(DEVICE),targets.to(DEVICE)\n",
    "    outputs = net(inputs)\n",
    "    normal_ls.append(loss_fn(outputs,targets).detach().cpu())\n",
    "    normal_pds.append(F.softmax(outputs,dim=1))\n",
    "    normal_tgts.append(targets)\n",
    "normal_ls = torch.cat(normal_ls)\n",
    "normal_tgts = torch.cat(normal_tgts)\n",
    "normal_pds = torch.cat(normal_pds)\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pds,tgts,ls = [],[],[]\n",
    "for batch_id, (inputs, targets,mask) in enumerate(dataloaders.forget):\n",
    "    inputs,targets = inputs.to(DEVICE),targets.to(DEVICE)\n",
    "    outputs = net(inputs)\n",
    "    loss = (outputs,targets)\n",
    "    ls.append(loss_fn(outputs,targets).detach().cpu())\n",
    "    pds.append(F.softmax(outputs,dim=1))\n",
    "    tgts.append(targets)\n",
    "pds = torch.cat(pds)\n",
    "tgts = torch.cat(tgts)\n",
    "ls = torch.cat(ls)\n",
    "\n",
    "\n",
    "from plots import plot_losses\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 6))\n",
    "test_losses = val_pds.max(dim=1)[0].detach().cpu().numpy()\n",
    "forget_losses = normal_pds.max(dim=1)[0].detach().cpu().numpy()\n",
    "X = (test_losses, forget_losses)\n",
    "weights = (np.ones_like(test_losses)/len(test_losses),\n",
    "            np.ones_like(forget_losses)/len(forget_losses))\n",
    "labels = (\"Non Training class\", \"Train classes\")\n",
    "bins = np.histogram(np.hstack(X), bins=20)[1]  # get the bin edges\n",
    "\n",
    "ax1.hist(X, density=False, alpha=0.5, bins=bins,\n",
    "    weights=weights, label=labels)\n",
    "\n",
    "ax1.set_ylabel(\"Percentage Samples\", fontsize=12)\n",
    "ax1.set_xlabel(\"Confidence\", fontsize=12)\n",
    "ax1.legend(frameon=False, fontsize=8)\n",
    "\n",
    "\n",
    "test_losses = val_ls.detach().cpu().numpy()\n",
    "forget_losses = normal_ls.detach().cpu().numpy()\n",
    "X = (test_losses, forget_losses)\n",
    "weights = (np.ones_like(test_losses)/len(test_losses),\n",
    "            np.ones_like(forget_losses)/len(forget_losses))\n",
    "labels = (\"Non Training class\", \"Train classes\")\n",
    "bins = np.histogram(np.hstack(X), bins=20)[1]  # get the bin edges\n",
    "\n",
    "\n",
    "ax2.hist(X, density=False, alpha=0.5, bins=bins,\n",
    "    weights=weights, label=labels)\n",
    "ax2.set_xlabel(\"Cross entropy loss\", fontsize=12)\n",
    "ax2.legend(frameon=False, fontsize=8)\n",
    "plt.savefig('results/scratch_loss.png', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
