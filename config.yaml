BASE_DIR : "unlearning"
DEVICE : 'cuda:1'
SEED : 42
BATCH_SIZE : 1024
DATA_PATH: 'data'
MODEL_DIR: 'models'

directory:
  RESULT_PATH: 'results'
  LOG_PATH: 'logs'
  LOGGER_PATH: 'logger.log'

data:
  num_workers: 16

model:
  name: 'resnet18' 
  model_args:
    num_classes: 10
  checkpoint: "/research/hal-gaudisac/unlearning/models/weights_resnet18_cifar10.pth"

trainer :
  checkpoint: '/research/hal-gaudisac/unlearning/models/weigths_resnet18.pt'
  verbose: True
  train: True
  log_freq: 20
  epochs: 1
  optimizer: 
    type: 'SGD'
    lr: 0.1
    momentum: 0.9
    weight_decay: 0.0001
  scheduler: 
    type: 'lr_scheduler.CosineAnnealingWarmRestarts'
    T_0: 10
    eta_min: 1e-6
    last_epoch: -1

attack_model:
  name: 'MLP-grad-mia'
  type: 'MiaModel'
  charecterstic: 'gradients'
  folds: 5
  model_args:
    model_config: [512,256,128,64,32,16,8]
    num_classes: 2
    
attack_trainer: 
  verbose: True
  epochs: 100
  optimizer:
    type: 'SGD'
    lr: 0.1
    momentum: 0.9
    weight_decay: 0.0001
  scheduler: 
    type: 'lr_scheduler.CosineAnnealingWarmRestarts'
    T_0: 10
    eta_min: 1e-6
    last_epoch: -1










#model:
#  name: 'ResNet' 
#  model_args:
#    block_config: [[3,64,1],[2,128,2],[2,256,2],[2,512,2]] [(1,8,1),(1,16,2),(1,32,2)] #model with 3 blocks of 16 channels with a stride of 1 and 2 blocks of 32 and 64 channels with a stride of 2 

#training :
#  checkpoint: '/research/hal-gaudisac/unlearning/models/weigths_ResNet_3x64x1-2x128x2-2x256x2-2x512x2.pt'
#  checkpoint: '/research/hal-gaudisac/unlearning/models/weigths_ResNet_1x8x1-1x16x2-1x32x2.pt'


#attack_model:
#  name: 'MLP-grad-mia'
#  type: 'MiaModel' LogisticRegression GradientBoostingClassifier
#  charecterstic: 'gradient' 
#  model_args:
#    model_config: [512,256,128,64,32,16,8] num_iters=1000